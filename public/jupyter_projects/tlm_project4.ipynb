{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Stock Price Prediction\n",
    "\n",
    "[ML Cookbook](https://www.ml-book.com) | [SLACK Channel](https://join.slack.com/t/mlckbk/shared_invite/zt-9qsjm911-6nSHAcCSjKfuHi972iEfEg)\n",
    "\n",
    "\n",
    "## About\n",
    "In this project you have to build a **time series forecasting model for predicting the price of Google stocks**.\n",
    "\n",
    "## Structure\n",
    "The project is split into 7 sections, each containing step-by-step instructions of what to do. These sections are the following:\n",
    "\n",
    "- Import the Libratries\n",
    "- Import the Datasets\n",
    "- Data Preprocessing\n",
    "- Data Overview\n",
    "- Model Building\n",
    "- Model Evaluation & Hyperparameter Tuning\n",
    "- Conclusion\n",
    "\n",
    "## Data\n",
    "There are 3 datasets provided that you should use for this project which represent the closing price of Google stocks for a 15 years period from 2006 to 2020:\n",
    "\n",
    "- *GOOGLE_stocks_2006_2010.csv*\n",
    "- *GOOGLE_stocks_2011_2015.csv*\n",
    "- *GOOGLE_stocks_2016_2020.csv*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import the Libraries\n",
    "Import the libraries needed (here you will also keep adding up the required libraries as you go further with this project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import the datasets\n",
    "\n",
    "Do the following:\n",
    "\n",
    "- **Step 1**: Import three dataframes as df1, df2 and df3 **(we did that for you)**\n",
    "- **Step 2:** See what the dataframes look like\n",
    "- **Step 3:** For each dataframe print its shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "Import three dataframes as df1, df2 and df3 **(we did that for you)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('https://raw.githubusercontent.com/the-learning-machine/data/master/tlm_project4/GOOGLE_stocks_2006_2010.csv')\n",
    "df2 = pd.read_csv('https://raw.githubusercontent.com/the-learning-machine/data/master/tlm_project4/GOOGLE_stocks_2011_2015.csv')\n",
    "df3 = pd.read_csv('https://raw.githubusercontent.com/the-learning-machine/data/master/tlm_project4/GOOGLE_stocks_2016_2020.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "See what the dataframes look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "For each dataframe print its shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing\n",
    "\n",
    "**Step 1:** Combine three datasets into one\n",
    "\n",
    "**Step 2:** Create a class called \"Prep\". Inside that class write functions that:\n",
    "\n",
    "- For a given year and month prints:\n",
    "    - minimum value\n",
    "    - mean value\n",
    "    - median value\n",
    "    - maximum value \n",
    "- Prints data types\n",
    "- Prints number of null values for each column\n",
    "\n",
    "**Step 3:** Explore the data: \n",
    "- Check dtypes using the Prep class, change data types if they don't look good to you\n",
    "- Take a look at the records between 2011 and 2015 years, does its order look good to you? If not, add to the Prep class a function to fix it\n",
    "- Are there any missing data in this dataset?\n",
    "\n",
    "**Step 4:** Impute missing records. There are multiple ways to do it in time series context. Feel free to select any of the methods suggested below or to use any other method. Add to the Prep class a function that imputes missing values in dataset.\n",
    "- Imputation by Linear Interpolation\n",
    "- Imputation by Last Observation Carried Forward\n",
    "- Imputation by Next Observation Carried Backward\n",
    "- Imputation by Simple Moving Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "Combine three datasets into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "Create a class called \"Prep\". Inside that class write functions that:\n",
    "\n",
    "- For a given year and month prints:\n",
    "    - minimum value\n",
    "    - mean value\n",
    "    - median value\n",
    "    - maximum value \n",
    "- Prints data types\n",
    "- Prints number of null values for each column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "Explore the data: \n",
    "- Check dtypes using the Prep class, change data types if they don't look good to you\n",
    "- Take a look at the records between 2011 and 2015 years, does its order look good to you? If not, add to the Prep class a function to fix it\n",
    "- Are there any missing data in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "Impute missing records. There are multiple ways to do it in time series context. Feel free to select any of the methods suggested below or to use any other method. Add to the Prep class a function that imputes missing values in dataset.\n",
    "- Imputation by Linear Interpolation\n",
    "- Imputation by Last Observation Carried Forward\n",
    "- Imputation by Next Observation Carried Backward\n",
    "- Imputation by Simple Moving Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Overview\n",
    "\n",
    "Observe the data:\n",
    "\n",
    "**Step 1:** Plot the graph of a stock price vs time.\n",
    "\n",
    "**Step 2:** Decompose time series data into [Trend, Cycle and Seasonality](https://en.wikipedia.org/wiki/Decomposition_of_time_series) and plot those graphs. Can you gain any insights or identify any patterns from there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "Plot the graph of a stock price vs time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "Decompose time series data into [Trend, Cycle and Seasonality](https://en.wikipedia.org/wiki/Decomposition_of_time_series) and plot those graphs. Can you gain any insights or identify any patterns from there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Building\n",
    "\n",
    "Do the following:\n",
    "\n",
    "**Step 1:** Split the data into train and test. Keep in mind that traditional splitting strategies that you might have used in classic regression / classification problems is not applicable for time series forecasting (try to think why?). Thus, we recommend using a strategy outlined [here (clickable)](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html).\n",
    "\n",
    "**Step 2:** We will use [ARIMA](https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima_model.ARIMA.html) model as it is simple yet powerful model that can result in good predictive power if treated correctly. This model has [3 main parameters](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average). This model works well when time series is stationary. You can estimate how stationary is the data via Augmented Dickey-Fuller (ADF) test. Calculate p-value for ADF test conducted on original data.\n",
    "\n",
    "**Step 3:** In case time series is non-stationary, try to experiment with the following transformations to make the time series look more stationary:\n",
    "- [Box-Cox Transformation](https://stats.stackexchange.com/questions/253917/why-use-differencing-and-box-cox-in-time-series#:~:text=1%20Answer&text=The%20Box-Cox%20transformation%20is,have%20a%20non-constant%20variance.)\n",
    "- [Differencing](https://people.duke.edu/~rnau/411diff.htm)\n",
    "\n",
    "The value of parameter *d* is the minimum number of differencing needed to make the series stationary.\n",
    "\n",
    "**Step 4:** Plot the [ACF (AutoCorrelation Function) graph](https://www.statsmodels.org/dev/generated/statsmodels.graphics.tsaplots.plot_acf.html) and select a starting value of *q* parameter. It should be equal to the first lag which ACF value is within the significance line minus 1. \n",
    "\n",
    "**Step 5:** Plot the [PACF (Partial AutoCorrelation Function) graph](https://www.statsmodels.org/dev/generated/statsmodels.graphics.tsaplots.plot_pacf.html) and select a starting value of *p* parameter. It should be equal to the first lag which ACF value is within the significance line minus 1. \n",
    "\n",
    "**Step 6:** Fit the [ARIMA](https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima_model.ARIMA.html) model using the parameters found above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "Split the data into train and test. Keep in mind that traditional splitting strategies that you might have used in classic regression / classification problems is not applicable for time series forecasting (try to think why?). Thus, we recommend using a strategy outlined [here (clickable)](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "We will use [ARIMA](https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima_model.ARIMA.html) model as it is simple yet powerful model that can result in good predictive power if treated correctly. This model has [3 main parameters](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average). This model works well when time series is stationary. You can estimate how stationary is the data via Augmented Dickey-Fuller (ADF) test. Calculate p-value for ADF test conducted on original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "In case time series is non-stationary, try to experiment with the following transformations to make the time series look more stationary:\n",
    "- [Box-Cox Transformation](https://stats.stackexchange.com/questions/253917/why-use-differencing-and-box-cox-in-time-series#:~:text=1%20Answer&text=The%20Box-Cox%20transformation%20is,have%20a%20non-constant%20variance.)\n",
    "- [Differencing](https://people.duke.edu/~rnau/411diff.htm)\n",
    "\n",
    "The value of parameter *d* is the minimum number of differencing needed to make the series stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "Plot the [ACF (AutoCorrelation Function) graph](https://www.statsmodels.org/dev/generated/statsmodels.graphics.tsaplots.plot_acf.html) and select a starting value of *q* parameter. It should be equal to the first lag which ACF value is within the significance line minus 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "Plot the [PACF (Partial AutoCorrelation Function) graph](https://www.statsmodels.org/dev/generated/statsmodels.graphics.tsaplots.plot_pacf.html) and select a starting value of *p* parameter. It should be equal to the first lag which ACF value is within the significance line minus 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6\n",
    "Fit the [ARIMA](https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima_model.ARIMA.html) model using the parameters found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Evaluation & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Call the `summary()` method on the fitted ARIMA model. \n",
    "\n",
    "**Step 2:** Take a look at the p-values of model coefficients that were outputted as a part of `summary()` output. If there are any p-value larger than 0.05, try to remove them from the model via adjusting parameters p and q (if possible).\n",
    "\n",
    "**Step 3:** If you decided to change values of p and q in Step 2, then re-train the model with these new parameters, then call the `summary()` method again and see, how [AIC](https://en.wikipedia.org/wiki/Akaike_information_criterion) and [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion) values changed. We would typically expect better model to have lower values of those coefficients.\n",
    "\n",
    "**Step 4:** Plot model's residuals. If the model is good, residuals should have zero mean and constant variance.\n",
    "\n",
    "**Step 5:** Plot Actuals VS Fitted graph to visually observe how well ARIMA models the actual data. You may find [this function](https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima_model.ARIMAResults.plot_predict.html#statsmodels.tsa.arima_model.ARIMAResults.plot_predict) to be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "Call the `summary()` method on the fitted ARIMA model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "Take a look at the p-values of model coefficients that were outputted as a part of `summary()` output. If there are any p-value larger than 0.05, try to remove them from the model via adjusting parameters p and q (if possible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "If you decided to change values of p and q in Step 2, then re-train the model with these new parameters, then call the `summary()` method again and see, how [AIC](https://en.wikipedia.org/wiki/Akaike_information_criterion) and [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion) values changed. We would typically expect better model to have lower values of those coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "Plot model's residuals. If the model is good, residuals should have zero mean and constant variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "Plot Actuals VS Fitted graph to visually observe how well ARIMA models the actual data. You may find [this function](https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima_model.ARIMAResults.plot_predict.html#statsmodels.tsa.arima_model.ARIMAResults.plot_predict) to be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Conclusion\n",
    "\n",
    "Summarize your **findings**. Did you manage to make the given time series (Google stock prices) stationary? What **data preprocessing** strategies have you used in order to get the best model? Which model has performed the best? \n",
    "\n",
    "Feel free to share/discuss your findings in our [Slack Channel](https://join.slack.com/t/mlcookbook/shared_invite/zt-eyz4czw4-l95j_2iuETCbVRPpgA3kWA)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "\n",
    "'''\n",
    "\n",
    "I used X model and achieved Y accuracy...\n",
    "I believe the model is reliable as I performed X feature selection technique...\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Extra Food for Thought"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might want to spend few minutes to explore the following topics for future use:\n",
    "\n",
    "- SARIMA + [Seasonal Differencing](https://people.duke.edu/~rnau/411sdif.htm) (in case there is a seasonality in data)\n",
    "- SARIMAX\n",
    "- Prophet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
